{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "535fd04c",
   "metadata": {},
   "source": [
    "# CIFAR-10: Object Recognition\n",
    "___\n",
    "\n",
    "\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2788e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15bc228",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbe821",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.CIFAR10('./data', train=True, download=True,  # Downloads into a directory ../data\n",
    "                               transform=transforms.ToTensor())\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, \n",
    "                                                             [int(len(train_dataset)*0.8), int(len(train_dataset)*0.2)], \n",
    "                                                             generator=torch.Generator().manual_seed(42))\n",
    "test_dataset = datasets.CIFAR10('./data', train=False, download=False,  # No need to download again\n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e81d61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_loop(model, batch_size=32, n_epochs=10, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Run a training loop based on the input model and associated parameters\n",
    "    \n",
    "    Parameters:\n",
    "        model: The input model to be trained\n",
    "        batch_size: Number of training points to include in batch\n",
    "        n_epochs: Number of epochs to train the model for\n",
    "        lr: Learning rate used in Adam optimizer\n",
    "        \n",
    "    \"\"\"\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Choose Adam as the optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Use the cross entropy loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # store metrics\n",
    "    train_loss_history = np.zeros([n_epochs, 1])\n",
    "    valid_accuracy_history = np.zeros([n_epochs, 1])\n",
    "    valid_loss_history = np.zeros([n_epochs, 1])\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Some layers, such as Dropout, behave differently during training\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # Erase accumulated gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(output, target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Weight update\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss_history[epoch] = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Track loss each epoch\n",
    "        print('Train Epoch: %d  Average loss: %.4f' %\n",
    "              (epoch + 1,  train_loss_history[epoch]))\n",
    "\n",
    "        # Putting layers like Dropout into evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        valid_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        # Turning off automatic differentiation\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                valid_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        valid_loss_history[epoch] = valid_loss / len(valid_loader.dataset)\n",
    "        valid_accuracy_history[epoch] = correct / len(valid_loader.dataset)\n",
    "\n",
    "        print('Valid set: Average loss: %.4f, Accuracy: %d/%d (%.4f)\\n' %\n",
    "              (valid_loss_history[epoch], correct, len(valid_loader.dataset),\n",
    "              100. * valid_accuracy_history[epoch]))\n",
    "    \n",
    "    return model, train_loss_history, valid_loss_history, valid_accuracy_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_performance(model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Test model performance on test dataset\n",
    "    \n",
    "    Parameters:\n",
    "        model: The model to be tested\n",
    "        batch_size: Number of training points to include in batch\n",
    "    \"\"\"\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "    # Putting layers like Dropout into evaluation mode\n",
    "    model.eval()\n",
    "    # Use the cross entropy loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Send model to appropriate device\n",
    "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    # Turning off automatic differentiation\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max class score\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: %.4f, Accuracy: %d/%d (%.4f)' %\n",
    "          (test_loss, correct, len(test_loader.dataset),\n",
    "          100. * test_accuracy))\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c262ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 6, 5), # (3, 32, 32) -> (6, 28, 28)\n",
    "    nn.BatchNorm2d(num_features=6),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2), # (6, 28, 28) -> (6, 14, 14)\n",
    "    nn.Conv2d(6, 16, 5), # (6, 14, 14) -> (16, 10, 10)\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2, 2), # (16, 10, 10) -> (16, 5, 5)\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 5 * 5, 120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),\n",
    "    nn.Dropout(p=0.1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6ac41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model, train_loss_history, valid_loss_history, valid_accuracy_history = run_training_loop(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
